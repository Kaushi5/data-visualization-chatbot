import os
import shutil
import pandas as pd
import gradio as gr
import subprocess
import matplotlib.pyplot as plt
from visualization_suggestions import generate_visualization_suggestions

# Paths for storing datasets and visualizations
RAW_DATA_PATH = "datasets/raw/"
VISUALIZATION_PATH = "visualizations/"
os.makedirs(RAW_DATA_PATH, exist_ok=True)
os.makedirs(VISUALIZATION_PATH, exist_ok=True)

# Global variables
chat_history = []
cleaned_dataset_path = None  # To store the path of the uploaded dataset


def process_dataset_with_ollama(file_path, model="llama2"):
    """
    Process the dataset using the Ollama model to generate a summary.

    Args:
        file_path (str): Path to the uploaded dataset.
        model (str): Ollama model to use.

    Returns:
        str: Summary generated by the LLM.
    """
    try:
        # Read dataset for preview
        dataset = pd.read_excel(file_path, engine="openpyxl")
        preview = dataset.head().to_string()

        # Prompt for the LLM
        prompt = (
            "Analyze the following dataset and generate a summary, including:\n"
            "- Key columns\n"
            "- Data types\n"
            "- Observations about its structure\n\n"
            f"Dataset preview:\n{preview}"
        )

        # Call Ollama CLI
        ollama_command = ["ollama", "run", model]
        process = subprocess.run(
            ollama_command,
            input=prompt,
            text=True,
            capture_output=True,
            encoding="utf-8",
        )

        # Check for errors
        if process.returncode != 0:
            raise Exception(f"Ollama Error: {process.stderr.strip()}")

        # Return the model's response
        return process.stdout.strip()

    except Exception as e:
        raise Exception(f"Error interacting with Ollama or processing the dataset: {str(e)}")


def handle_uploaded_file(file_path):
    global uploaded_file_path, dataset
    try:
        uploaded_file_path = file_path
        dataset = pd.read_excel(file_path)
        return [
            {"role": "assistant", "content": "Dataset successfully uploaded! You can now generate visualizations or insights."}
        ]
    except Exception as e:
        return [
            {"role": "assistant", "content": f"An error occurred while uploading the dataset: {str(e)}"}
        ]





def generate_visualization(file_path, request):
    """
    Generates visualizations based on user requests.

    Args:
        file_path (str): Path to the uploaded dataset.
        request (str): Visualization request (e.g., "Bar Chart: X=Category, Y=Sales").

    Returns:
        str: Path to the generated visualization image.
        str: High-level insight explaining the visualization.
    """
    try:
        # Read the dataset
        dataset = pd.read_excel(file_path, engine="openpyxl")

        # Strip column names to remove trailing spaces
        dataset.columns = dataset.columns.str.strip()

        # Parse request for chart type and axes
        chart_type = request.split(":")[0].strip()
        x_col = request.split("X=")[1].split(",")[0].strip()
        y_col = request.split("Y=")[1].strip()

        # Check if columns exist
        if x_col not in dataset.columns or y_col not in dataset.columns:
            raise KeyError(f"One or both of the specified columns '{x_col}' or '{y_col}' do not exist in the dataset.")

        # Ensure numeric columns are properly converted
        dataset[y_col] = pd.to_numeric(dataset[y_col], errors="coerce")

        # Generate visualization
        plt.figure(figsize=(10, 6))
        if "Bar" in chart_type:
            dataset.groupby(x_col)[y_col].sum().plot(kind="bar")
            plt.title(f"Bar Chart: {y_col} by {x_col}")
            insight = f"This bar chart shows the distribution of {y_col} across {x_col}."
        elif "Line" in chart_type:
            dataset.plot(x=x_col, y=y_col, kind="line")
            plt.title(f"Line Chart: {y_col} over {x_col}")
            insight = f"This line chart shows trends in {y_col} over {x_col}."
        elif "Scatter" in chart_type:
            dataset.plot.scatter(x=x_col, y=y_col)
            plt.title(f"Scatter Plot: {y_col} vs {x_col}")
            insight = f"This scatter plot visualizes the relationship between {x_col} and {y_col}."
        else:
            raise ValueError("Unsupported chart type. Please select Bar, Line, or Scatter.")

        # Save the visualization
        visualization_file = f"{chart_type}_{x_col}_vs_{y_col}.png"
        visualization_path = os.path.join(os.getcwd(), visualization_file)
        plt.savefig(visualization_path)
        plt.close()

        return visualization_path, insight

    except KeyError as e:
        raise ValueError(f"Invalid column name: {e}")
    except Exception as e:
        raise Exception(f"Error generating visualization: {str(e)}")




def handle_visualization_request(request):
    """
    Handles visualization requests for supported chart types.
    
    Args:
        request (str): Visualization request (e.g., "Bar Chart: X=Category, Y=Sales").
        
    Returns:
        str: Success or error message.
    """
    global uploaded_file_path

    try:
        # Ensure a dataset is uploaded
        if not uploaded_file_path or not os.path.exists(uploaded_file_path):
            return [{"role": "assistant", "content": "No dataset available. Please upload a dataset first."}]

        # Load the dataset
        dataset = pd.read_excel(uploaded_file_path, engine="openpyxl")

        # Parse the visualization request
        if ":" not in request or "=" not in request:
            return [{"role": "assistant", "content": "Invalid request format. Use: Chart Type: X=Column, Y=Column"}]

        viz_type, axis_info = request.split(":", 1)
        x_col = axis_info.split(",")[0].split("=")[1].strip()
        y_col = axis_info.split(",")[1].split("=")[1].strip()

        # Validate column existence
        if x_col not in dataset.columns or y_col not in dataset.columns:
            return [{"role": "assistant", "content": f"Error: Column(s) '{x_col}' or '{y_col}' not found in the dataset."}]

        # Prepare the dataset for numeric operations if required
        try:
            dataset[y_col] = pd.to_numeric(dataset[y_col], errors="coerce")
            dataset = dataset.dropna(subset=[x_col, y_col])  # Remove rows with NaN
        except Exception as e:
            return [{"role": "assistant", "content": f"Error processing data: {str(e)}"}]

        # Generate the visualization
        plt.figure(figsize=(10, 6))
        if "Bar" in viz_type:
            dataset.groupby(x_col)[y_col].sum().plot(kind="bar", color="skyblue", edgecolor="black")
            plt.title(f"Bar Chart: {x_col} vs {y_col}")
            plt.xlabel(x_col)
            plt.ylabel(y_col)

        elif "Bubble" in viz_type:
            if "Size" not in axis_info:
                return [{"role": "assistant", "content": "Bubble Chart requires a 'Size' column. Use format: X=Col1, Y=Col2, Size=Col3"}]
            size_col = axis_info.split(",")[2].split("=")[1].strip()
            if size_col not in dataset.columns:
                return [{"role": "assistant", "content": f"Error: Size column '{size_col}' not found in the dataset."}]
            plt.scatter(
                dataset[x_col],
                dataset[y_col],
                s=dataset[size_col] * 10,
                alpha=0.5,
                color="blue",
                edgecolors="w",
            )
            plt.title(f"Bubble Chart: {x_col} vs {y_col}")
            plt.xlabel(x_col)
            plt.ylabel(y_col)

        elif "Line" in viz_type:
            dataset.plot(x=x_col, y=y_col, kind="line", marker="o", linestyle="-", color="green")
            plt.title(f"Line Chart: {x_col} vs {y_col}")
            plt.xlabel(x_col)
            plt.ylabel(y_col)

        elif "Box" in viz_type:
            dataset[[y_col]].plot(kind="box", vert=False, patch_artist=True, notch=True, color=dict(boxes="red"))
            plt.title(f"Box Plot: {y_col}")
            plt.xlabel(y_col)

        else:
            return [{"role": "assistant", "content": f"Visualization type '{viz_type}' is not supported."}]

        # Save the visualization
        output_file = "generated_visualization.png"
        plt.savefig(output_file, dpi=300)
        plt.close()

        return [{"role": "assistant", "content": f"Visualization saved as '{output_file}'. Check the folder for the output."}]

    except Exception as e:
        return [{"role": "assistant", "content": f"An error occurred while generating the visualization: {str(e)}"}]







def suggest_visualizations_with_ollama(file_path, model="llama2"):
    """
    Suggest visualizations for the dataset using the Ollama model.

    Args:
        file_path (str): Path to the uploaded dataset.
        model (str): Ollama model to use.

    Returns:
        str: Visualization suggestions generated by the LLM.
    """
    try:
        # Read dataset for column analysis
        dataset = pd.read_excel(file_path, engine="openpyxl")
        columns = dataset.columns.tolist()

        # Generate a prompt for visualization suggestions
        prompt = (
            "Based on the following dataset, suggest appropriate visualizations:\n"
            "- Include chart types (e.g., Bar Chart, Scatter Plot, Line Chart).\n"
            "- Specify relevant columns for X and Y axes.\n"
            "- Provide a high-level explanation of each visualization.\n\n"
            f"Dataset Columns:\n{', '.join(columns)}\n\n"
            "Consider trends, distributions, and correlations."
        )

        # Call Ollama CLI
        ollama_command = ["ollama", "run", model]
        process = subprocess.run(
            ollama_command,
            input=prompt,
            text=True,
            capture_output=True,
            encoding="utf-8",
        )

        # Check for errors
        if process.returncode != 0:
            raise Exception(f"Ollama Error: {process.stderr.strip()}")

        # Return the model's response
        llm_response = process.stdout.strip()
        return llm_response

    except Exception as e:
        raise Exception(f"Error interacting with Ollama or suggesting visualizations: {str(e)}")


def handle_visualization_suggestions():
    global uploaded_file_path
    if uploaded_file_path and os.path.exists(uploaded_file_path):
        try:
            # Pass the file path to generate_visualization_suggestions
            suggestions = generate_visualization_suggestions(uploaded_file_path)
            return [
                {"role": "assistant", "content": "Based on the provided dataset, here are some visualization suggestions:\n" + suggestions}
            ]
        except Exception as e:
            return [
                {"role": "assistant", "content": f"An error occurred while generating visualization suggestions: {str(e)}"}
            ]
    else:
        return [
            {"role": "assistant", "content": "No dataset available for visualization suggestions. Please upload a dataset first."}
        ]




def get_visualization_suggestions():
    global uploaded_file_path, chat_history
    try:
        # Ensure a dataset is uploaded
        if not uploaded_file_path or not os.path.exists(uploaded_file_path):
            chat_history.append({"role": "assistant", "content": "No dataset available for visualization suggestions. Please upload a dataset first."})
            return chat_history

        # Load the dataset
        dataset = pd.read_excel(uploaded_file_path)

        # Generate suggestions based on dataset content
        suggestions = """
        Based on the provided dataset, here are some visualization suggestions:
        1. Bar Chart: Compare Category vs. MRP Old.
        2. Line Chart: Plot Weight vs. Final MRP Old.
        3. Scatter Plot: Analyze TP vs. Weight.
        """

        # Append suggestions to chat history
        chat_history.append({"role": "assistant", "content": suggestions})
        return chat_history

    except Exception as e:
        chat_history.append({"role": "assistant", "content": f"An error occurred while generating suggestions: {str(e)}"})
        return chat_history

def handle_visualization_insight_request(uploaded_visual_path):
    """
    Generates insights for the uploaded visualization using Ollama LLM.

    Args:
        uploaded_visual_path (str): Path to the uploaded visualization image.

    Returns:
        List[Dict]: Chatbot response with the generated insights.
    """
    try:
        # Extract visualization metadata (e.g., title, axes labels)
        visualization_name = os.path.basename(uploaded_visual_path)

        # Prepare prompt for Ollama
        prompt = (
            f"I have uploaded a visualization image named '{visualization_name}'.\n"
            "Analyze this visualization and generate insights, focusing on key patterns, trends, and outliers.\n"
            "Provide actionable insights that would benefit a business analyzing this data."
        )

        # Call Ollama CLI with the prompt
        ollama_command = ["ollama", "run", "llama2"]
        process = subprocess.run(
            ollama_command,
            input=prompt,
            text=True,
            capture_output=True,
            encoding="utf-8",
        )

        # Check for errors in the subprocess
        if process.returncode != 0:
            raise Exception(f"Ollama Error: {process.stderr.strip()}")

        # Retrieve and process the response from Ollama
        llm_response = process.stdout.strip()

        # Save insights to a file
        insights_file_path = os.path.join(os.getcwd(), "visualization_insights.txt")
        with open(insights_file_path, "w") as file:
            file.write(llm_response)

        return [{"role": "assistant", "content": f"Insights saved to '{insights_file_path}'.\n\n{llm_response}"}]

    except Exception as e:
        return [{"role": "assistant", "content": f"An error occurred: {str(e)}"}]



def handle_combined_visualization_suggestions(visual_paths, insight_paths):
    """
    Generate visualization suggestions using uploaded visualizations and insights.
    Args:
        visual_paths (List[str]): Paths to uploaded visualization images.
        insight_paths (List[str]): Paths to corresponding insights.
    Returns:
        List[Dict]: Suggestions for combined visualizations.
    """
    try:
        # Prepare a prompt for Ollama LLM
        visual_names = ", ".join([os.path.basename(path) for path in visual_paths])
        insights_summary = ""
        for path in insight_paths:
            with open(path, "r") as file:
                insights_summary += f"\n- {file.read()}"

        prompt = (
            f"I have uploaded the following visualizations: {visual_names}.\n"
            "Here are the insights from these visualizations:\n"
            f"{insights_summary}\n"
            "Based on these, suggest combined visualizations that can provide holistic insights.\n"
            "Include chart types (e.g., Bar Chart, Line Chart, etc.) and axes columns."
        )

        # Call Ollama LLM
        ollama_command = ["ollama", "run", "llama2"]
        process = subprocess.run(
            ollama_command,
            input=prompt,
            text=True,
            capture_output=True,
            encoding="utf-8",
        )

        # Check for errors in the subprocess
        if process.returncode != 0:
            raise Exception(f"Ollama Error: {process.stderr.strip()}")

        # Return suggestions
        llm_response = process.stdout.strip()
        return [{"role": "assistant", "content": f"Visualization suggestions:\n\n{llm_response}"}]

    except Exception as e:
        return [{"role": "assistant", "content": f"An error occurred: {str(e)}"}]


def handle_combined_visualization_request(visual_paths, insight_paths, request):
    """
    Generate a combined visualization based on user-selected suggestions.
    Args:
        visual_paths (List[str]): Paths to uploaded visualization images.
        insight_paths (List[str]): Paths to corresponding insights.
        request (str): Visualization request based on suggestions.
    Returns:
        List[Dict]: Path to the combined visualization and generated insights.
    """
    try:
        # Combine the visualizations and insights into one prompt
        visual_names = ", ".join([os.path.basename(path) for path in visual_paths])
        insights_summary = ""
        for path in insight_paths:
            with open(path, "r") as file:
                insights_summary += f"\n- {file.read()}"

        prompt = (
            f"I have uploaded the following visualizations: {visual_names}.\n"
            "Here are the insights from these visualizations:\n"
            f"{insights_summary}\n"
            f"Now create a combined visualization as per the request: {request}\n"
            "Generate actionable insights that benefit the business."
        )

        # Call Ollama LLM
        ollama_command = ["ollama", "run", "llama2"]
        process = subprocess.run(
            ollama_command,
            input=prompt,
            text=True,
            capture_output=True,
            encoding="utf-8",
        )

        # Check for errors in the subprocess
        if process.returncode != 0:
            raise Exception(f"Ollama Error: {process.stderr.strip()}")

        # Retrieve the insights and save them to a file
        llm_response = process.stdout.strip()
        insights_file_path = os.path.join(os.getcwd(), "combined_visualization_insights.txt")
        with open(insights_file_path, "w") as file:
            file.write(llm_response)

        return [{"role": "assistant", "content": f"Combined insights saved to '{insights_file_path}'.\n\n{llm_response}"}]

    except Exception as e:
        return [{"role": "assistant", "content": f"An error occurred: {str(e)}"}]


def generate_combined_visualization_dynamic(visuals_paths, insights_paths):
    """
    Generate a combined visualization dynamically based on uploaded visuals and insights.

    Args:
        visuals_paths (list): List of paths to the uploaded visualization images.
        insights_paths (list): List of paths to the uploaded insights text files.

    Returns:
        str: Path to the generated combined visualization image.
    """
    try:
        # Extract dynamic information from insights
        extracted_info = []
        for insight_path in insights_paths:
            with open(insight_path, "r") as file:
                content = file.read()
                extracted_info.append(content)

        # Generate a combined visualization title and legend dynamically
        title = "Combined Visualization Based on Insights"
        legend_labels = [os.path.basename(path).split(".")[0] for path in visuals_paths]

        # Placeholder combined visualization logic
        fig, ax = plt.subplots(figsize=(10, 6))

        # Iterate over uploaded visuals for dynamic combination
        for idx, visual_path in enumerate(visuals_paths):
            # Simulate dynamic plotting (e.g., just show as bars or trends)
            ax.bar([idx], [idx + 1], label=legend_labels[idx])

        # Dynamic Title and Legend
        ax.set_title(title)
        ax.set_xlabel("Dynamic X-Axis")
        ax.set_ylabel("Dynamic Y-Axis")
        ax.legend()

        # Save the visualization
        combined_visual_path = os.path.join(os.getcwd(), "combined_visualization_dynamic.png")
        plt.savefig(combined_visual_path)
        plt.close()

        # Generate insights dynamically
        combined_insights = "Combined insights based on the following:\n\n"
        for idx, info in enumerate(extracted_info):
            combined_insights += f"Insight {idx + 1}:\n{info}\n\n"

        # Save combined insights
        combined_insights_path = os.path.join(os.getcwd(), "combined_visualization_insights_dynamic.txt")
        with open(combined_insights_path, "w") as insights_file:
            insights_file.write(combined_insights)

        return combined_visual_path, combined_insights_path

    except Exception as e:
        raise Exception(f"Error generating combined visualization dynamically: {str(e)}")


def handle_combined_visualization_request_dynamic(visuals_paths, insights_paths, request):
    """
    Handle the combined visualization request dynamically.

    Args:
        visuals_paths (list): List of paths to uploaded visualizations.
        insights_paths (list): List of paths to uploaded insights.
        request (str): Visualization request based on suggestions.

    Returns:
        List[Dict]: Chatbot response, combined visualization file path, combined insights file path.
    """
    try:
        # Generate the combined visualization and insights
        combined_visual_path, combined_insights_path = generate_combined_visualization_dynamic(
            visuals_paths, insights_paths
        )

        # Return properly formatted chatbot message and file paths
        chatbot_message = {
            "role": "assistant",
            "content": f"Combined visualization and insights generated successfully. Visualization type: {request}.",
        }
        return [
            chatbot_message,
            combined_visual_path,  # Path to the combined visualization file
            combined_insights_path,  # Path to the combined insights file
        ]
    except Exception as e:
        # Return error message for chatbot and empty file outputs
        chatbot_message = {
            "role": "assistant",
            "content": f"An error occurred while generating combined outputs: {str(e)}",
        }
        return [
            chatbot_message,
            None,  # No file to return
            None,  # No insights to return
        ]







def generate_dynamic_visualization_with_ollama(visuals_paths, insights_paths, model="llama2"):
    """
    Generate a dynamic visualization based on insights and visuals using Ollama LLM.

    Args:
        visuals_paths (list): List of paths to the uploaded visualization images.
        insights_paths (list): List of paths to the uploaded insights text files.
        model (str): The LLM model to use (default: "llama2").

    Returns:
        str: Path to the generated visualization image.
        str: Path to the combined insights text file.
    """
    try:
        # Combine insights into a single string
        combined_insights = "Combined insights from multiple inputs:\n\n"
        for idx, insight_path in enumerate(insights_paths):
            with open(insight_path, "r") as file:
                content = file.read()
                combined_insights += f"Insight {idx + 1}:\n{content}\n\n"

        # Save combined insights to a file
        combined_insights_path = os.path.join(os.getcwd(), "combined_visualization_insights.txt")
        with open(combined_insights_path, "w") as file:
            file.write(combined_insights)

        # Prepare a prompt for Ollama to generate the visualization code
        prompt = (
            f"Here are insights derived from visualizations:\n\n{combined_insights}\n\n"
            "Based on these insights, please suggest and provide Python code to dynamically create a visualization "
            "that combines the data represented in the uploaded visualizations and insights. "
            "The visualization should be saved as an image file in the current working directory."
        )

        # Call Ollama CLI
        ollama_command = ["ollama", "run", model]
        process = subprocess.run(
            ollama_command,
            input=prompt,
            text=True,
            capture_output=True,
            encoding="utf-8",
        )

        # Check for errors in the subprocess
        if process.returncode != 0:
            raise Exception(f"Ollama Error: {process.stderr.strip()}")

        # Extract Python code from LLM response
        llm_response = process.stdout.strip()
        if "```python" in llm_response and "```" in llm_response:
            code_snippet = llm_response.split("```python")[1].split("```")[0].strip()
        else:
            raise Exception("No valid Python code snippet found in the LLM response.")

        # Execute the Python code snippet dynamically
        exec_globals = {"os": os, "plt": plt, "sns": sns, "pd": pd, "np": np}
        exec_locals = {}
        exec(code_snippet, exec_globals, exec_locals)

        # Extract the saved visualization path from executed code
        visualization_path = exec_locals.get("visualization_path")
        if not visualization_path or not os.path.exists(visualization_path):
            raise Exception("Visualization image was not generated or saved.")

        return visualization_path, combined_insights_path

    except Exception as e:
        raise Exception(f"Error generating dynamic visualization: {str(e)}")

def handle_combined_insight_request_with_ollama(visual_paths, insight_paths, model="llama2"):
    """
    Generate combined insights using uploaded visualizations and individual insights with Ollama LLM.

    Args:
        visual_paths (list): Paths to uploaded visualization images.
        insight_paths (list): Paths to corresponding insights text files.
        model (str): Ollama model to use.

    Returns:
        list: Chatbot response with confirmation of saved file.
    """
    try:
        # Read individual insights from uploaded text files
        individual_insights = "Individual Insights:\n\n"
        for idx, insight_path in enumerate(insight_paths):
            with open(insight_path, "r") as file:
                content = file.read()
                individual_insights += f"Insight {idx + 1}:\n{content}\n\n"

        # Summarize visualizations for the prompt
        visual_names = ", ".join([os.path.basename(path) for path in visual_paths])
        combined_prompt = (
            f"I have uploaded the following visualizations: {visual_names}.\n"
            "Here are the individual insights from these visualizations:\n"
            f"{individual_insights}\n"
            "Based on this, generate combined insights focusing on patterns, trends, and actionable business recommendations.\n"
        )

        # Call Ollama CLI with the prompt
        ollama_command = ["ollama", "run", model]
        process = subprocess.run(
            ollama_command,
            input=combined_prompt,
            text=True,
            capture_output=True,
            encoding="utf-8",
        )

        # Check for errors from the subprocess
        if process.returncode != 0:
            raise Exception(f"Ollama Error: {process.stderr.strip()}")

        # Retrieve and process the response from Ollama
        llm_response = process.stdout.strip()

        # Save combined insights to a text file in the current working directory
        combined_insights_path = os.path.join(os.getcwd(), "combined_insights.txt")
        with open(combined_insights_path, "w") as file:
            file.write(llm_response)

        # Return chatbot message confirming the file was saved
        return [
            {"role": "assistant", "content": f"Combined insights generated and saved to '{combined_insights_path}'."},
        ]

    except Exception as e:
        # Return error message in case of any issue
        return [
            {"role": "assistant", "content": f"An error occurred while generating combined insights: {str(e)}"}
        ]



# Gradio Interface
with gr.Blocks() as interface:
    chatbot = gr.Chatbot(label="Data Visualization Chatbot", type="messages")
    
    # Single Dataset and Visualization Inputs
    file_upload = gr.File(label="Upload Excel Dataset", type="filepath")
    suggestion_button = gr.Button("Get Visualization Suggestions")
    visualization_request = gr.Textbox(label="Enter your visualization request")
    generate_button = gr.Button("Submit Visualization Request")
    visualization_upload = gr.File(label="Upload Generated Visualization", type="filepath")
    insight_button = gr.Button("Get Insights from Visualization")

    # Combined Visualizations and Insights Inputs
    combined_visuals_upload = gr.Files(
        label="Upload Visualizations (Multiple)", 
        type="filepath", 
        file_types=[".png", ".jpg", ".jpeg"], 
        elem_id="combined_visuals"
    )
    combined_insights_upload = gr.Files(
        label="Upload Insights (Multiple)", 
        type="filepath", 
        file_types=[".txt"], 
        elem_id="combined_insights"
    )
    combined_insight_button = gr.Button("Generate Combined Insights")

    # File upload and visualization generation
    file_upload.change(handle_uploaded_file, inputs=[file_upload], outputs=chatbot)
    suggestion_button.click(handle_visualization_suggestions, outputs=chatbot)
    generate_button.click(handle_visualization_request, inputs=[visualization_request], outputs=chatbot)

    # Upload visualization and generate insights
    insight_button.click(handle_visualization_insight_request, inputs=[visualization_upload], outputs=chatbot)

    # Combined visualization generation
    combined_insight_button.click(
        handle_combined_insight_request_with_ollama,
        inputs=[combined_visuals_upload, combined_insights_upload],
        outputs=[chatbot],
    )


if __name__ == "__main__":
    interface.launch()



